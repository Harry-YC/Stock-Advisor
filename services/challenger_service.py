"""
Red Team Challenger Service for Palliative Surgery GDG

Generates high-quality challenging questions about GDG recommendations
to stress-test guideline recommendations before finalization.

The challenger focuses on:
1. Expert disagreements - areas where panel didn't align
2. Evidence gaps - claims not validated by literature (PMIDs)
3. Hidden assumptions - unstated premises in recommendations
4. Patient selection - who might NOT benefit from the recommendation
5. Risk-benefit thresholds - decision points that need clarity

Usage:
    from services.challenger_service import ChallengerService, generate_challenges

    # Quick usage
    challenges = generate_challenges(recommendation, expert_responses, conflicts)

    # Full control
    challenger = ChallengerService()
    result = challenger.challenge_recommendation(recommendation, expert_responses, conflicts, evidence_gaps)
"""

import json
import logging
import re
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Optional

from config import settings

logger = logging.getLogger(__name__)


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class ChallengeQuestion:
    """A single challenging question generated by the red team."""
    question: str
    category: str  # assumption, evidence, patient_selection, threshold, risk, feasibility
    targets: str   # Which weak point this addresses
    rationale: str # Why this question matters for the guideline

    def to_dict(self) -> Dict:
        return {
            "question": self.question,
            "category": self.category,
            "targets": self.targets,
            "rationale": self.rationale,
        }


@dataclass
class ChallengeOutput:
    """Complete output from the challenger service."""
    analysis: str  # Brief reasoning about weak points identified
    questions: List[ChallengeQuestion]
    model_used: str
    generated_at: str

    # Optional metadata
    conflicts_count: int = 0
    evidence_gaps_count: int = 0

    def to_dict(self) -> Dict:
        return {
            "analysis": self.analysis,
            "questions": [q.to_dict() for q in self.questions],
            "model_used": self.model_used,
            "generated_at": self.generated_at,
            "conflicts_count": self.conflicts_count,
            "evidence_gaps_count": self.evidence_gaps_count,
        }


# =============================================================================
# PROMPTS
# =============================================================================

CHALLENGER_SYSTEM_PROMPT = """You are a skeptical red team reviewer for clinical guideline recommendations in palliative surgery.

Think like:
- A guideline methodologist checking for logical gaps
- A surgeon who has seen complications from similar recommendations
- A palliative care physician advocating for patient autonomy
- An ethicist questioning assumptions about benefit

Your job: Challenge the GDG panel's recommendation with precise, actionable questions.

IMPORTANT - Focus on areas where:
1. Experts ALREADY disagreed (detected conflicts) - these are gold
2. Evidence is WEAK or MISSING (evidence gaps without PMIDs) - probe weak foundations
3. Patient selection criteria are vague - who should NOT receive this intervention?
4. Risk-benefit thresholds are unclear - when does harm outweigh benefit?
5. Hidden assumptions could be wrong - expose unstated premises

Style: Direct, specific, professional. Each question should:
- Be answerable (not rhetorical)
- Target a specific weak point in the recommendation
- Matter for real clinical decision-making

Do NOT simply agree with the panel. Your job is to find flaws that could harm patients if left unaddressed."""


def build_challenger_prompt(
    recommendation: str,
    question: str,
    expert_summary: str,
    conflicts_summary: str,
    evidence_gaps_summary: str,
    key_findings: List[str],
) -> str:
    """Build the user prompt for the challenger."""

    # Format key findings
    findings_text = ""
    if key_findings:
        findings_text = "\n".join([f"- {f}" for f in key_findings[:5]])

    return f"""## GDG RECOMMENDATION TO CHALLENGE

**Clinical Question:** {question}

**Panel Recommendation:**
{recommendation}

## KEY FINDINGS FROM EXPERT PANEL
{findings_text if findings_text else "No key findings extracted."}

## EXPERT DISAGREEMENTS (focus here - these are high-value targets!)
{conflicts_summary if conflicts_summary else "No explicit conflicts detected."}

## EVIDENCE GAPS (claims NOT validated by PMIDs)
{evidence_gaps_summary if evidence_gaps_summary else "No evidence gaps identified."}

## EXPERT PERSPECTIVES SUMMARY
{expert_summary if expert_summary else "No expert perspectives available."}

---

## YOUR TASK

**Step 1:** Identify the 3 weakest points in this recommendation. Look for:
- Vague patient selection criteria (who should NOT get this?)
- Unvalidated claims (no PMID support)
- Unresolved expert disagreements
- Implicit assumptions about prognosis, functional status, or goals of care
- Missing risk-benefit thresholds

**Step 2:** For each weak point, formulate ONE sharp challenging question.

**Categories to use:**
- `assumption` - Hidden premises that could be wrong
- `evidence` - Weak or missing PMID support
- `patient_selection` - Who should/shouldn't receive this intervention
- `threshold` - Decision criteria, when to operate vs not
- `risk` - Complications, morbidity that may be underweighted
- `feasibility` - Can this be implemented in practice?

**Output as JSON:**
```json
{{
  "analysis": "Brief 2-3 sentence summary of the 3 weakest points you identified",
  "questions": [
    {{
      "question": "The specific challenging question",
      "category": "assumption|evidence|patient_selection|threshold|risk|feasibility",
      "targets": "Which weak point this addresses (e.g., 'No PMID for 30-day mortality claim')",
      "rationale": "Why this question matters for the guideline (1 sentence)"
    }}
  ]
}}
```

Generate 3-5 questions. Quality over quantity."""


# =============================================================================
# CHALLENGER SERVICE
# =============================================================================

class ChallengerService:
    """
    Red Team Challenger that critiques GDG panel recommendations.

    Uses Gemini (via llm_router) for chain-of-thought reasoning
    to generate relevant, important questions for guideline refinement.
    """

    def __init__(self, model: str = None):
        """
        Initialize the challenger service.

        Args:
            model: LLM model to use. Defaults to settings.REASONING_MODEL or gemini-3-pro-preview
        """
        self.model = model or getattr(settings, 'REASONING_MODEL', 'gemini-3-pro-preview')
        self._client = None

    @property
    def client(self):
        """Lazy-load the LLM router."""
        if self._client is None:
            from services.llm_router import LLMRouter
            self._client = LLMRouter()
        return self._client

    def challenge_recommendation(
        self,
        recommendation: str,
        question: str = "",
        expert_responses: Dict = None,
        conflicts: List[str] = None,
        evidence_gaps: List[str] = None,
        key_findings: List[str] = None,
    ) -> ChallengeOutput:
        """
        Generate red team challenges for a GDG recommendation.

        Args:
            recommendation: The panel's recommendation text
            question: Original clinical question
            expert_responses: Dict of {expert_name: response_content}
            conflicts: List of detected disagreements/conflicts
            evidence_gaps: List of evidence gap descriptions
            key_findings: List of key findings from the panel

        Returns:
            ChallengeOutput with analysis and questions
        """
        # Format context
        expert_summary = self._format_expert_responses(expert_responses or {})
        conflicts_summary = self._format_conflicts(conflicts or [])
        evidence_gaps_summary = self._format_evidence_gaps(evidence_gaps or [])

        # Build prompt
        user_prompt = build_challenger_prompt(
            recommendation=recommendation,
            question=question,
            expert_summary=expert_summary,
            conflicts_summary=conflicts_summary,
            evidence_gaps_summary=evidence_gaps_summary,
            key_findings=key_findings or [],
        )

        # Call LLM
        try:
            response = self._invoke_llm(user_prompt)
            result = self._parse_response(response)

            # Add metadata
            result.model_used = self.model
            result.generated_at = datetime.now().isoformat()
            result.conflicts_count = len(conflicts or [])
            result.evidence_gaps_count = len(evidence_gaps or [])

            logger.info(f"Challenger generated {len(result.questions)} questions")
            return result

        except Exception as e:
            logger.error(f"Challenger failed: {e}")
            return self._fallback_response(str(e))

    def _invoke_llm(self, user_prompt: str) -> str:
        """Invoke the LLM with the challenger prompts."""
        messages = [
            {"role": "system", "content": CHALLENGER_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]

        # Use the LLM router
        response = self.client.chat(
            messages=messages,
            model=self.model,
            temperature=0.7,  # Some creativity for alternative thinking
            max_tokens=2000,
        )

        return response

    def _parse_response(self, response_text: str) -> ChallengeOutput:
        """Parse the LLM response into ChallengeOutput."""
        # Clean markdown code blocks
        text = response_text.strip()
        if '```json' in text:
            text = text.split('```json')[1].split('```')[0].strip()
        elif '```' in text:
            text = text.split('```')[1].split('```')[0].strip()

        try:
            data = json.loads(text)
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parse failed, attempting recovery: {e}")
            # Try to extract JSON from response
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                data = json.loads(json_match.group())
            else:
                raise ValueError(f"Could not parse JSON from response: {response_text[:500]}")

        # Build questions
        questions = []
        for q in data.get('questions', []):
            questions.append(ChallengeQuestion(
                question=q.get('question', ''),
                category=q.get('category', 'general'),
                targets=q.get('targets', ''),
                rationale=q.get('rationale', ''),
            ))

        return ChallengeOutput(
            analysis=data.get('analysis', ''),
            questions=questions,
            model_used=self.model,
            generated_at=datetime.now().isoformat(),
        )

    def _format_expert_responses(self, expert_responses: Dict) -> str:
        """Format expert responses for context."""
        if not expert_responses:
            return ""

        lines = []
        for expert, response in list(expert_responses.items())[:8]:  # Limit to 8 experts
            if isinstance(response, dict):
                content = response.get('content', '')
            else:
                content = str(response)

            # Truncate for context
            summary = content[:300] + "..." if len(content) > 300 else content
            lines.append(f"**{expert}:** {summary}")

        return "\n\n".join(lines)

    def _format_conflicts(self, conflicts: List) -> str:
        """Format conflicts for the prompt."""
        if not conflicts:
            return ""

        lines = []
        for i, c in enumerate(conflicts[:5], 1):  # Limit to 5
            if isinstance(c, dict):
                # Structured conflict
                expert = c.get('expert', 'Unknown')
                position = c.get('position', c.get('description', ''))
                dissent_type = c.get('type', 'concern')
                lines.append(f"{i}. **{expert}** ({dissent_type}): {position}")
            else:
                # Simple string
                lines.append(f"{i}. {c}")

        return "\n".join(lines)

    def _format_evidence_gaps(self, gaps: List) -> str:
        """Format evidence gaps for the prompt."""
        if not gaps:
            return ""

        lines = []
        for i, g in enumerate(gaps[:5], 1):  # Limit to 5
            if isinstance(g, dict):
                gap = g.get('gap', g.get('description', 'Unknown gap'))
                action = g.get('suggested_action', '')
                lines.append(f"{i}. {gap}")
                if action:
                    lines.append(f"   Suggested action: {action}")
            else:
                lines.append(f"{i}. {g}")

        return "\n".join(lines)

    def _fallback_response(self, error: str) -> ChallengeOutput:
        """Generate fallback response if LLM fails."""
        return ChallengeOutput(
            analysis=f"Challenge generation failed: {error}. Using generic questions.",
            questions=[
                ChallengeQuestion(
                    question="Which patients would NOT benefit from this intervention, and how do we identify them pre-operatively?",
                    category="patient_selection",
                    targets="Patient selection criteria",
                    rationale="Clear exclusion criteria prevent harm to unsuitable patients",
                ),
                ChallengeQuestion(
                    question="What is the specific PMID evidence for the mortality/morbidity rates cited?",
                    category="evidence",
                    targets="Outcome claims",
                    rationale="GRADE requires traceable evidence for all numeric claims",
                ),
                ChallengeQuestion(
                    question="At what life expectancy threshold does the risk-benefit balance shift against intervention?",
                    category="threshold",
                    targets="Decision thresholds",
                    rationale="Clear thresholds guide real-world application of the guideline",
                ),
            ],
            model_used="fallback",
            generated_at=datetime.now().isoformat(),
        )


# =============================================================================
# CONVENIENCE FUNCTIONS
# =============================================================================

def generate_challenges(
    recommendation: str,
    question: str = "",
    expert_responses: Dict = None,
    conflicts: List = None,
    evidence_gaps: List = None,
    key_findings: List[str] = None,
    model: str = None,
) -> ChallengeOutput:
    """
    Convenience function to generate challenges.

    Args:
        recommendation: The panel's recommendation text
        question: Original clinical question
        expert_responses: Dict of expert responses
        conflicts: Detected conflicts/disagreements
        evidence_gaps: Evidence gaps
        key_findings: Key findings from panel
        model: LLM model (defaults to REASONING_MODEL)

    Returns:
        ChallengeOutput with analysis and questions
    """
    challenger = ChallengerService(model=model)
    return challenger.challenge_recommendation(
        recommendation=recommendation,
        question=question,
        expert_responses=expert_responses,
        conflicts=conflicts,
        evidence_gaps=evidence_gaps,
        key_findings=key_findings,
    )
