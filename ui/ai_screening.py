"""
AI Screening UI Module

Handles the AI-powered paper screening interface.
"""

import streamlit as st
from config import settings
from core.ai_screener import screen_papers_batch, get_screening_summary
from core.database import AIScreeningDAO

# Visualization imports
try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

def render_ai_screening(ai_screening_dao: AIScreeningDAO):
    """
    Renders the AI Screening tab.
    
    Args:
        ai_screening_dao: Data Access Object for AI screening results
    """
    st.title("ğŸ¤– AI Screening")

    if not st.session_state.current_project_name:
        st.info("ğŸ‘ˆ Select a project to begin")
        return

    # Direct Input for Independence
    st.caption("Paste abstracts to screen.")
    direct_input_text = st.text_area(
        "Abstracts",
        placeholder="Paste abstracts to screen...",
        height=100,
        key="screening_direct_input",
        label_visibility="collapsed"
    )
    
    # ... (Logic for paper loading remains similar but cleaner) ...
    # [Logic abbreviated for visual diff tool - assume standard loading logic here]
    
    citations = []
    source_type = "search"
    if direct_input_text.strip():
        source_type = "direct"
        entries = direct_input_text.strip().split('\n\n')
        for i, entry in enumerate(entries):
            if len(entry) > 20:
                lines = entry.split('\n')
                citations.append({
                    'pmid': f"man-{i}", 'title': lines[0][:200], 
                    'abstract': "\n".join(lines[1:]) if len(lines)>1 else entry,
                    'year': '2024'
                })
        if citations: st.success(f"{len(citations)} loaded")
    elif st.session_state.search_results and st.session_state.search_results.get('citations'):
        citations = st.session_state.search_results['citations']
    
    if not citations:
        st.caption("No papers loaded to screen.")
        return

    st.markdown(f"**Papers:** {len(citations)}")

    col1, col2 = st.columns(2)
    with col1:
        confidence_threshold = st.slider("Min Confidence", 50, 95, 80)
    with col2:
        screening_scenario = st.selectbox("Focus", ["General", "Target Validation", "PK/PD", "Biomarker", "Competitive Intel"])

    if not settings.OPENAI_API_KEY:
        st.error("Key Missing")
        st.stop()

    # Run Screening Button
    if st.button("ğŸš€ Run AI Screening", type="primary", use_container_width=True):
        # ai_screening_results initialized in core/state_manager.py

        progress_bar = st.progress(0)
        status_text = st.empty()

        def update_progress(current, total):
            progress_bar.progress(current / total)
            status_text.text(f"Screening paper {current}/{total}...")

        with st.spinner("Running AI screening..."):
            try:
                # Convert Citation objects to dicts if needed
                papers_to_screen = []
                for c in citations:
                    if hasattr(c, 'pmid'):
                        papers_to_screen.append({
                            'pmid': c.pmid,
                            'title': c.title,
                            'abstract': c.abstract,
                            'authors': c.authors,
                            'journal': c.journal,
                            'year': c.year
                        })
                    else:
                        papers_to_screen.append(c)

                results = screen_papers_batch(
                    papers=papers_to_screen,
                    search_query=search_query if source_type == "search" else f"Screening for {screening_scenario}",
                    openai_api_key=settings.OPENAI_API_KEY,
                    model=settings.SCREENING_MODEL,
                    confidence_threshold=confidence_threshold,
                    progress_callback=update_progress
                )

                st.session_state.ai_screening_results = results

                # Save to database (skip if manual-input to avoid polluting DB with fake PMIDs, or handle gracefully)
                if source_type == "search":
                    for paper in results:
                        ai_screening_dao.save_screening(
                            project_id=st.session_state.current_project_id,
                            pmid=paper.get('pmid', ''),
                            decision=paper.get('ai_decision', 'review'),
                            confidence=paper.get('ai_confidence', 0),
                            reasoning=paper.get('ai_reasoning', '')
                        )

                progress_bar.progress(1.0)
                status_text.text("Screening complete!")
                st.success(f"âœ… Screened {len(results)} papers")

            except Exception as e:
                st.error(f"âŒ Screening failed: {str(e)}")

    # Display Results
    if 'ai_screening_results' in st.session_state and st.session_state.ai_screening_results:
        results = st.session_state.ai_screening_results

        st.markdown("---")
        st.subheader("ğŸ“Š Screening Results")

        # Summary stats
        summary = get_screening_summary(results)

        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Total", summary['total'])
        col2.metric("Include", summary['include'], delta=None)
        col3.metric("Exclude", summary['exclude'], delta=None)
        col4.metric("Review", summary['review'], delta=None)

        # Visualization: Pie chart of screening decisions
        if PLOTLY_AVAILABLE and summary['total'] > 0:
            col_chart, col_stats = st.columns([2, 1])

            with col_chart:
                # Create pie chart
                labels = ['Include', 'Exclude', 'Needs Review']
                values = [summary['include'], summary['exclude'], summary['review']]
                colors = ['#2ca02c', '#d62728', '#ff7f0e']  # green, red, orange

                fig = go.Figure(data=[go.Pie(
                    labels=labels,
                    values=values,
                    marker_colors=colors,
                    hole=0.4,  # Donut chart
                    textinfo='label+percent',
                    textposition='outside'
                )])

                fig.update_layout(
                    title="Screening Decision Distribution",
                    showlegend=True,
                    legend=dict(orientation="h", yanchor="bottom", y=-0.2, xanchor="center", x=0.5),
                    margin=dict(t=60, b=60, l=20, r=20),
                    paper_bgcolor='rgba(0,0,0,0)',
                    height=300
                )

                st.plotly_chart(fig, use_container_width=True)

            with col_stats:
                st.markdown("**Screening Stats**")

                # Calculate percentages
                total = summary['total']
                if total > 0:
                    include_pct = summary['include'] / total * 100
                    exclude_pct = summary['exclude'] / total * 100
                    review_pct = summary['review'] / total * 100

                    st.markdown(f"- **Include rate:** {include_pct:.1f}%")
                    st.markdown(f"- **Exclude rate:** {exclude_pct:.1f}%")
                    st.markdown(f"- **Review rate:** {review_pct:.1f}%")

                    # Average confidence
                    confidences = [p.get('ai_confidence', 0) for p in results if p.get('ai_confidence')]
                    if confidences:
                        avg_conf = sum(confidences) / len(confidences)
                        st.markdown(f"- **Avg confidence:** {avg_conf:.1f}%")

                    # Screening efficiency
                    auto_decided = summary['include'] + summary['exclude']
                    efficiency = auto_decided / total * 100 if total > 0 else 0
                    st.markdown(f"- **Auto-decided:** {efficiency:.1f}%")

        # Filter tabs
        tab1, tab2, tab3, tab4 = st.tabs(["All", "Include", "Exclude", "Needs Review"])

        def display_papers(papers, allow_override=True):
            for i, paper in enumerate(papers):
                with st.expander(f"**{paper.get('title', 'No title')[:80]}...**"):
                    st.markdown(f"**PMID:** {paper.get('pmid', 'N/A')}")
                    decision = paper.get('ai_decision', 'N/A').lower()
                    if decision == 'include':
                        st.markdown(f"**Decision:** :green[INCLUDE] (Confidence: {paper.get('ai_confidence', 0)}%)")
                    elif decision == 'exclude':
                        st.markdown(f"**Decision:** :red[EXCLUDE] (Confidence: {paper.get('ai_confidence', 0)}%)")
                    elif decision == 'review':
                        st.markdown(f"**Decision:** :orange[NEEDS REVIEW] (Confidence: {paper.get('ai_confidence', 0)}%)")
                    else:
                        st.markdown(f"**Decision:** {decision.upper()} (Confidence: {paper.get('ai_confidence', 0)}%)")
                    
                    st.markdown(f"**Reasoning:** {paper.get('ai_reasoning', 'N/A')}")

                    if paper.get('abstract'):
                        st.markdown("**Abstract:**")
                        st.caption(paper['abstract'][:500] + "..." if len(paper.get('abstract', '')) > 500 else paper['abstract'])

                    if allow_override:
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            if st.button("âœ… Include", key=f"inc_{paper.get('pmid')}_{i}"):
                                ai_screening_dao.update_decision(
                                    st.session_state.current_project_id,
                                    paper.get('pmid'),
                                    'include'
                                )
                                paper['ai_decision'] = 'include'
                                st.rerun()
                        with col2:
                            if st.button("âŒ Exclude", key=f"exc_{paper.get('pmid')}_{i}"):
                                ai_screening_dao.update_decision(
                                    st.session_state.current_project_id,
                                    paper.get('pmid'),
                                    'exclude'
                                )
                                paper['ai_decision'] = 'exclude'
                                st.rerun()
                        with col3:
                            if st.button("ğŸ” Review", key=f"rev_{paper.get('pmid')}_{i}"):
                                ai_screening_dao.update_decision(
                                    st.session_state.current_project_id,
                                    paper.get('pmid'),
                                    'review'
                                )
                                paper['ai_decision'] = 'review'
                                st.rerun()

        with tab1:
            display_papers(results)

        with tab2:
            include_papers = [p for p in results if p.get('ai_decision') == 'include']
            if include_papers:
                display_papers(include_papers)
            else:
                st.info("No papers marked for inclusion yet.")

        with tab3:
            exclude_papers = [p for p in results if p.get('ai_decision') == 'exclude']
            if exclude_papers:
                display_papers(exclude_papers)
            else:
                st.info("No papers marked for exclusion yet.")

        with tab4:
            review_papers = [p for p in results if p.get('ai_decision') == 'review']
            if review_papers:
                display_papers(review_papers)
            else:
                st.info("No papers need manual review.")
